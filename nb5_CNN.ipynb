{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 3 Types of CNNs: Basic, Lenet, Alexnet\n",
    "Base code was adapted form the following tutorial:\n",
    "https://www.analyticsvidhya.com/blog/2021/08/beginners-guide-to-convolutional-neural-network-with-implementation-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the coco annotations\n",
    "import numpy as np\n",
    "\n",
    "# visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load in the names_labels_df csv and turn it into a df\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the kappa score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate AUROC\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Images and Resize Them If Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### State Your Paths ###################################\n",
    "\n",
    "patches_image_directory = \"your_path_here/All_Patches/\"\n",
    "split_data_folder = \"your_path_here/Train_test_develop_split_all_combined/\"\n",
    "\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Load in the data ######################################\n",
    "\n",
    "# Load in the names_images_labels_df csv and turn it into a df\n",
    "train_names_labels_df = pd.read_csv(f\"{split_data_folder}tra_val_names_labels_df.csv\")\n",
    "test_names_labels_df = pd.read_csv(f\"{split_data_folder}dev_names_labels_df.csv\")\n",
    "\n",
    "# Get the lists of file names and labels\n",
    "train_names = train_names_labels_df['patch_name'].tolist()\n",
    "train_labels = train_names_labels_df['label'].tolist()\n",
    "\n",
    "# Get the lists of file names and labels for the testing data\n",
    "test_names = test_names_labels_df['patch_name'].tolist()\n",
    "test_labels = test_names_labels_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split ratio\n",
    "split_ratio = 0.8\n",
    "\n",
    "##################### Here I use the Real Dataset ################################\n",
    "\n",
    "# Split your data into training and validation sets with stratified randomization\n",
    "new_train_names, new_val_file_names, new_train_labels, new_val_labels = train_test_split(\n",
    "    train_names, train_labels, test_size=1 - split_ratio, random_state=42, stratify=train_labels) \n",
    "\n",
    "#################### Here I use the Real Dataset ##################################\n",
    "\n",
    "# Create empty NumPy arrays for your images \n",
    "image_size = (224, 224, 1) \n",
    "# image_size = (28, 28, 1) \n",
    "# image_size = (32, 32, 1) \n",
    "\n",
    "x_train = np.empty((len(new_train_names), *image_size), dtype=np.float32)\n",
    "x_val = np.empty((len(new_val_file_names), *image_size), dtype=np.float32)\n",
    "x_test = np.empty((len(test_names), *image_size), dtype=np.float32)\n",
    "\n",
    "# Load and preprocess the training images\n",
    "for i, image_name in enumerate(new_train_names):\n",
    "    image_path = os.path.join(patches_image_directory, image_name)\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # decode the image keeping only the first channel\n",
    "    image = tf.image.decode_image(image, channels=1)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    # image = tf.image.resize(image, (28, 28))\n",
    "    # image = tf.image.resize(image, (32, 32))\n",
    "    # image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    x_train[i] = image.numpy().astype(np.float32)  # Convert to NumPy array and cast to float32\n",
    "\n",
    "# Load and preprocess the validation images\n",
    "for i, image_name in enumerate(new_val_file_names):\n",
    "    image_path = os.path.join(patches_image_directory, image_name)\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=1)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    # image = tf.image.resize(image, (28, 28))\n",
    "    # image = tf.image.resize(image, (32, 32))\n",
    "    # image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    x_val[i] = image.numpy().astype(np.float32)  # Convert to NumPy array and cast to float32\n",
    "\n",
    "# Load and preprocess the testing images\n",
    "for i, image_name in enumerate(test_names):\n",
    "    image_path = os.path.join(patches_image_directory, image_name)\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=1)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    # image = tf.image.resize(image, (28, 28))\n",
    "    # image = tf.image.resize(image, (32, 32))\n",
    "    # image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    x_test[i] = image.numpy().astype(np.float32)  # Convert to NumPy array and cast to float32\n",
    "    \n",
    "# Where: \"categories\":[{\"id\":1,\"name\":\"Full\",\"supercategory\":\"\"},{\"id\":2,\"name\":\"Partial\",\"supercategory\":\"\"},{\"id\":3,\"name\":\"Empty\",\"supercategory\":\"\"}]\n",
    "\n",
    "# Convert the lists of labels to NumPy arrays. Convert each to an integer\n",
    "y_train = np.array(new_train_labels, dtype=np.float32) -1\n",
    "y_val = np.array(new_val_labels, dtype=np.float32) -1\n",
    "y_test = np.array(test_labels, dtype=np.float32) -1\n",
    "\n",
    "# y_train = np.array(new_train_labels, dtype=np.int32) - 1\n",
    "# y_val = np.array(new_val_labels, dtype=np.int32) - 1\n",
    "# y_test = np.array(test_labels, dtype=np.int32) - 1\n",
    "\n",
    "train = x_train, y_train\n",
    "val = x_val, y_val\n",
    "\n",
    "load_data = train, val\n",
    "\n",
    "# print the shapes of all the data\n",
    "y_train.shape, y_val.shape, y_test.shape, x_train.shape, x_val.shape, x_test.shape\n",
    "\n",
    "# (2m 43s) with metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Basic Model  ###########################################\n",
    "\n",
    "# np.savez_compressed(f\"{split_data_folder}train_images_labels_28x28smallimgs.npz\", x_train=x_train, y_train=y_train)\n",
    "# np.savez_compressed(f\"{split_data_folder}val_images_labels_28x28smallimgs.npz\", x_val=x_val, y_val=y_val)\n",
    "# np.savez_compressed(f\"{split_data_folder}test_images_labels_28x28smallimgs.npz\", x_test=x_test, y_test=y_test)\n",
    "\n",
    "########################################### Lenet  ###############################################\n",
    "\n",
    "# np.savez_compressed(f\"{split_data_folder}train_images_labels_32x32smallimgs.npz\", x_train=x_train, y_train=y_train)\n",
    "# np.savez_compressed(f\"{split_data_folder}val_images_labels_32x32smallimgs.npz\", x_val=x_val, y_val=y_val)\n",
    "# np.savez_compressed(f\"{split_data_folder}test_images_labels_32x32smallimgs.npz\", x_test=x_test, y_test=y_test)\n",
    "\n",
    "###########################################  Alexnet ###############################################\n",
    "\n",
    "np.savez_compressed(f\"{split_data_folder}train_images_labels_1ch.npz\", x_train=x_train, y_train=y_train)\n",
    "np.savez_compressed(f\"{split_data_folder}val_images_labels_1ch.npz\", x_val=x_val, y_val=y_val)\n",
    "np.savez_compressed(f\"{split_data_folder}test_images_labels_1ch.npz\", x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load in the .npz files for the basic model\n",
    "# train_data = np.load(f\"{split_data_folder}train_images_labels_28x28smallimgs.npz\")\n",
    "# x_train = train_data['x_train']\n",
    "# y_train = train_data['y_train']\n",
    "# val_data = np.load(f\"{split_data_folder}val_images_labels_28x28smallimgs.npz\")\n",
    "# x_val = val_data['x_val']\n",
    "# y_val = val_data['y_val']\n",
    "# test_data = np.load(f\"{split_data_folder}test_images_labels_28x28smallimgs.npz\")\n",
    "# x_test = test_data['x_test']\n",
    "# y_test = test_data['y_test']\n",
    "# print(\"script message: loaded in all data\")\n",
    "\n",
    "# # load in the .npz files for lenet\n",
    "# train_data = np.load(f\"{split_data_folder}train_images_labels_32x32smallimgs.npz\")\n",
    "# x_train = train_data['x_train']\n",
    "# y_train = train_data['y_train']\n",
    "# val_data = np.load(f\"{split_data_folder}val_images_labels_32x32smallimgs.npz\")\n",
    "# x_val = val_data['x_val']\n",
    "# y_val = val_data['y_val']\n",
    "# test_data = np.load(f\"{split_data_folder}test_images_labels_32x32smallimgs.npz\")\n",
    "# x_test = test_data['x_test']\n",
    "# y_test = test_data['y_test']\n",
    "# print(\"script message: loaded in all data\")\n",
    "\n",
    "# load in the .npz files for alexnet\n",
    "train_data = np.load(f\"{split_data_folder}train_images_labels_1ch.npz\")\n",
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "val_data = np.load(f\"{split_data_folder}val_images_labels_1ch.npz\")\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "test_data = np.load(f\"{split_data_folder}test_images_labels_1ch.npz\")\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "print(\"script message: loaded in all data\")\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print unique values in y_train\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train[0][0][0][0]))\n",
    "print(type(y_train[0]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train[0][0][0][0])\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt.imshow(x_train[5])\n",
    "plt.show()\n",
    "# Show the label\n",
    "print(y_train[5])\n",
    "print(x_train[5].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28, 1)))  \n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell is adapted from this tutorial: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(Input(shape=(28, 28, 1)))  \n",
    "  \n",
    "  # Tune the number of filters in the Conv2D layer. Choose an optimal value between 6-384\n",
    "  hp_filters = hp.Int('filters', min_value=32, max_value=256, step=32)\n",
    "  hp_kernel_size = hp.Choice('kernel_size', values=[2, 3, 5])\n",
    "  model.add(Conv2D(filters=hp_filters, kernel_size=hp_kernel_size, activation='relu')) \n",
    "  \n",
    "  # Tune the pool_size and the strides\n",
    "  hp_pool_size = hp.Choice('pool_size', values=[2, 3])\n",
    "  hp_strides = hp.Choice('strides', values=[1, 2])\n",
    "  model.add(MaxPool2D(pool_size=hp_pool_size, strides=hp_strides))\n",
    "  \n",
    "  \n",
    "  model.add(Flatten())\n",
    "  \n",
    "  # Tune the number of units in the Dense layers. Choose an optimal value between 10-1000\n",
    "  hp_units = hp.Int('units', min_value=100, max_value=500, step=10)\n",
    "  model.add(Dense(units=hp_units, activation='relu'))\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer. Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(32, 32, 1))) \n",
    "model.add(Conv2D(filters=6, kernel_size=5, padding='valid', activation='sigmoid'))\n",
    "model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=5, padding='valid', activation='sigmoid'))\n",
    "model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='sigmoid'))\n",
    "model.add(Dense(84, activation='sigmoid'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(32, 32, 1))) \n",
    "    \n",
    "    # Tune the number of filters in the Conv2D layer. Choose an optimal value between 6-384\n",
    "    hp_filters1 = hp.Int('filters1', min_value=6, max_value=9, step=3)\n",
    "    hp_kernel_size1 = hp.Choice('kernel_size1', values=[3, 5])\n",
    "    model.add(Conv2D(filters=hp_filters1, kernel_size=hp_kernel_size1, padding='valid', activation='sigmoid')) \n",
    "    \n",
    "    # Tune the pool_size and the strides\n",
    "    hp_pool_size1 = hp.Choice('pool_size1', values=[2])\n",
    "    hp_strides1 = hp.Choice('strides1', values=[2])\n",
    "    model.add(MaxPool2D(pool_size=hp_pool_size1, strides=hp_strides1))\n",
    "    \n",
    "    hp_filters2 = hp.Int('filters2', min_value=12, max_value=20, step=4)\n",
    "    hp_kernel_size2 = hp.Choice('kernel_size2', values=[3, 5])\n",
    "    model.add(Conv2D(filters=hp_filters2, kernel_size=hp_kernel_size2, padding='valid', activation='sigmoid'))\n",
    "    \n",
    "    hp_pool_size2 = hp.Choice('pool_size2', values=[2])\n",
    "    hp_strides2 = hp.Choice('strides2', values=[2])\n",
    "    model.add(MaxPool2D(pool_size=hp_pool_size2, strides=hp_strides2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    hp_units1 = hp.Int('units1', min_value=100, max_value=400, step=10)\n",
    "    model.add(Dense(units=hp_units1, activation='sigmoid'))\n",
    "    \n",
    "    hp_units2 = hp.Int('units2', min_value=80, max_value=100, step=4)\n",
    "    model.add(Dense(units=hp_units2, activation='sigmoid'))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(224, 224, 1)))  \n",
    "model.add(Conv2D(filters=96, kernel_size=11, strides=4, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=3, strides=2))\n",
    "model.add(Conv2D(filters=256, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=3, strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=3, padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=384, kernel_size=3, padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same',activation='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=3, strides=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(224, 224, 1)))\n",
    "    \n",
    "    hp_filters1 = hp.Int('filters1', min_value=78, max_value=120, step=6)\n",
    "    hp_kernel_size1 = hp.Choice('kernel_size1', values=[2, 6, 11])\n",
    "    model.add(Conv2D(filters=hp_filters1, kernel_size=hp_kernel_size1, strides=4, padding='valid', activation='relu'))\n",
    "    \n",
    "    hp_pool_size1 = hp.Choice('pool_size1', values=[2, 3])\n",
    "    hp_strides1 = hp.Choice('strides1', values=[1, 2])\n",
    "    model.add(MaxPool2D(pool_size=hp_pool_size1, strides=hp_strides1, padding='valid'))\n",
    "    \n",
    "    hp_filters2 = hp.Int('filters2', min_value=156, max_value=356, step=50)\n",
    "    hp_kernel_size2 = hp.Choice('kernel_size2', values=[2, 3, 5])\n",
    "    model.add(Conv2D(filters=hp_filters2, kernel_size=hp_kernel_size2, padding='same', activation='relu'))\n",
    "    \n",
    "    hp_pool_size2 = hp.Choice('pool_size2', values=[2, 3])\n",
    "    hp_strides2 = hp.Choice('strides2', values=[1, 2])\n",
    "    model.add(MaxPool2D(pool_size=hp_pool_size2, strides=hp_strides2, padding='valid'))\n",
    "\n",
    "    hp_filters3 = hp.Int('filters3', min_value=324, max_value=420, step=12)\n",
    "    hp_kernel_size3 = hp.Choice('kernel_size3', values=[2, 3])\n",
    "    model.add(Conv2D(filters=hp_filters3, kernel_size=hp_kernel_size3, padding='same',activation='relu'))\n",
    "    model.add(Conv2D(filters=hp_filters3, kernel_size=hp_kernel_size3, padding='same',activation='relu'))\n",
    "    \n",
    "    hp_filters4 = hp.Int('filters4', min_value=156, max_value=356, step=50)\n",
    "    hp_kernel_size4 = hp.Choice('kernel_size4', values=[2, 3])\n",
    "    model.add(Conv2D(filters=hp_filters4, kernel_size=hp_kernel_size4, padding='same',activation='relu'))\n",
    "\n",
    "    hp_pool_size3 = hp.Choice('pool_size3', values=[2, 3])\n",
    "    hp_strides3 = hp.Choice('strides3', values=[1, 2])\n",
    "    model.add(MaxPool2D(pool_size=hp_pool_size3, strides=hp_strides3, padding='valid'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    hp_units1 = hp.Int('units1', min_value=1000, max_value=2000, step=100)\n",
    "    model.add(Dense(units=hp_units1, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=hp_units1, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WITH Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_trials=15,\n",
    "                     overwrite=True)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_split= 0.2\n",
    "train_imageslen = len(x_train)*(1-validation_split)\n",
    "steps_per_epoch = int(train_imageslen // batch_size)\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=7, validation_split=validation_split, batch_size=batch_size, steps_per_epoch=steps_per_epoch, shuffle=True, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# uncomment the needed model below \n",
    "\n",
    "######################################## Basic Model  ###########################################\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. The optimal:\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size')}\n",
    "# 3) pool size in the MaxPool2D layer: {best_hps.get('pool_size')}\n",
    "# 4) strides in the MaxPool2D layer: {best_hps.get('strides')}\n",
    "# 1) Number of units in the first densely-connected layer: {best_hps.get('units')} \n",
    "# 2) Learning rate for the optimizer: {best_hps.get('learning_rate')}\n",
    "# \"\"\")\n",
    "\n",
    "########################################### Lenet  ###############################################\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. The optimal:\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters1')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size1')}\n",
    "# 3) pool size in the MaxPool2D layer: {best_hps.get('pool_size1')}\n",
    "# 4) strides in the MaxPool2D layer: {best_hps.get('strides1')}\n",
    "\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters2')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size2')}\n",
    "# 3) pool size in the MaxPool2D layer: {best_hps.get('pool_size2')}\n",
    "# 4) strides in the MaxPool2D layer: {best_hps.get('strides2')}\n",
    "\n",
    "# 1) Number of units in the first densely-connected layer: {best_hps.get('units1')} \n",
    "# 2) Number of units in the first densely-connected layer: {best_hps.get('units2')} \n",
    "\n",
    "# 1) Learning rate for the optimizer: {best_hps.get('learning_rate')}\n",
    "# \"\"\")\n",
    "\n",
    "###########################################  Alexnet ###############################################\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. The optimal:\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters1')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size1')}\n",
    "# 3) pool size in the MaxPool2D layer: {best_hps.get('pool_size1')}\n",
    "# 4) strides in the MaxPool2D layer: {best_hps.get('strides1')}\n",
    "\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters2')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size2')}\n",
    "# 3) pool size in the MaxPool2D layer: {best_hps.get('pool_size2')}\n",
    "# 4) strides in the MaxPool2D layer: {best_hps.get('strides2')}\n",
    "\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters3')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size3')}\n",
    "# 1) filters in the Conv2D layer: {best_hps.get('filters4')}\n",
    "# 2) kernel size in the Conv2D layer: {best_hps.get('kernel_size4')}\n",
    "# 3) pool size in the MaxPool2D layer: {best_hps.get('pool_size3')}\n",
    "# 4) strides in the MaxPool2D layer: {best_hps.get('strides3')}\n",
    "\n",
    "# 5) Number of units densely-connected layers: {best_hps.get('units1')} \n",
    "\n",
    "# 6) Learning rate for the optimizer: {best_hps.get('learning_rate')}\n",
    "# \"\"\")\n",
    "\n",
    "## 42m 0.2s - 133m 32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(x_train, y_train, epochs=15, validation_split=0.2, batch_size=32)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WithOUT Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train,y_train,epochs=10, validation_data=(x_val,y_val), batch_size=32)\n",
    "model.fit(x_train,y_train,epochs=15, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that x_val is of type float32\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = y_test\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Show the accuracy of the model\n",
    "accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Calculate the kappa score\n",
    "kappa = cohen_kappa_score(y_test, np.argmax(y_pred, axis=1))\n",
    "print(f\"Kappa: {kappa:.3f}\")\n",
    "\n",
    "# Compute the selectivity for empty capsids\n",
    "empty_selectivity = confusion_matrix(y_test, np.argmax(y_pred, axis=1))[2, 2] / np.sum(confusion_matrix(y_test, np.argmax(y_pred, axis=1))[2])\n",
    "print(f\"The selectivity for empty capsids is {empty_selectivity}\")\n",
    "\n",
    "# Calculate the AUROC \n",
    "# use label_binarize for multi-class data\n",
    "y_val_binarized = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "y_pred_binarized = tf.keras.utils.to_categorical(np.argmax(y_pred, axis=1), num_classes=3)\n",
    "auroc = roc_auc_score(y_val_binarized, y_pred_binarized, multi_class='ovr')\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "\n",
    "# show a confusion matrix \n",
    "cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Full', 'Partial', 'Empty'])\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Full', 'Partial', 'Empty', 'Agg', 'Ice', 'Broken', 'Backg'])\n",
    "\n",
    "disp.plot(cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
