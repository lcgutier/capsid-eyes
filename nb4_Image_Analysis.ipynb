{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will create patches from the annotated coco data \n",
    "1) Load in the coco annotations and labels\n",
    "2) load in the images that are labeled\n",
    "3) create patches from the images using the coco labels and save to a new file\n",
    "4) Create a new csv of the images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the coco annotations\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Image analysis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "#using sikit image to compute the HOG features\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Use SIFT to extract features from the images\n",
    "# Use K means clustering to cluster the images into 3 or 7 clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "################################### Machine Learning ###################################\n",
    "\n",
    "# Use a SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "\n",
    "# Train a decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Print the decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "# Computing the kappa score \n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate the AUROC \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define refit strategy\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### State Your Paths ###################################\n",
    "\n",
    "patches_image_directory = \"your_path_here/All_Patches_resized/\"\n",
    "split_data_folder = \"your_path_here/Train_test_develop_split_combined_background/\"\n",
    "model_checkpoints = \"your_path_here/Model_Checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Load in the Real Data #################################\n",
    "\n",
    "train_names_labels_df = pd.read_csv(f\"{split_data_folder}tra_val_names_labels_df.csv\")\n",
    "test_names_labels_df = pd.read_csv(f\"{split_data_folder}dev_names_labels_df.csv\")\n",
    "\n",
    "# Get the lists of file names and labels\n",
    "train_names = train_names_labels_df['patch_name'].tolist()\n",
    "train_labels = train_names_labels_df['label'].tolist()\n",
    "train_images = [cv2.imread(f\"{patches_image_directory}{i}\") for i in train_names]\n",
    "# resize the images to 224x224\n",
    "train_images = [cv2.resize(i, (224, 224)) for i in train_images]\n",
    "\n",
    "# # Get the lists of file names and labels for the testing data\n",
    "test_names = test_names_labels_df['patch_name'].tolist()\n",
    "test_labels = test_names_labels_df['label'].tolist()\n",
    "test_images = [cv2.imread(f\"{patches_image_directory}{i}\") for i in test_names]\n",
    "# resize the images to 224x224\n",
    "test_images = [cv2.resize(i, (224, 224)) for i in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the unique values of train_labels\n",
    "print(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shapes of the images\n",
    "print(f\"test images shape: {test_images[14].shape}\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "############################ Load in the split smooth synthetic data  ##############################\n",
    "\n",
    "smooth_synthetic_data= \"your_path_here/Synthetic_Data_smooth/\"\n",
    "\n",
    "# Load in the synthetic data and convert it to a list\n",
    "smooth_synthetic_train_images = np.load(f\"{smooth_synthetic_data}smooth_synthetic_train_images.npy\")\n",
    "smooth_synthetic_train_labels = np.load(f\"{smooth_synthetic_data}smooth_synthetic_train_labels.npy\")\n",
    "\n",
    "smooth_synthetic_dev_images = np.load(f\"{smooth_synthetic_data}smooth_synthetic_dev_images.npy\")\n",
    "smooth_synthetic_dev_labels = np.load(f\"{smooth_synthetic_data}smooth_synthetic_dev_labels.npy\")\n",
    "\n",
    "train_labels = smooth_synthetic_train_labels\n",
    "train_images = smooth_synthetic_train_images\n",
    "test_labels = smooth_synthetic_dev_labels\n",
    "test_images = smooth_synthetic_dev_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the number of patches in the training data\n",
    "print(f\"There are {len(train_labels)} training patches.\")\n",
    "print(f\"There are {len(test_labels)} testing patches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the images\n",
    "def plot_images(images, labels, num_images, capsid_type):\n",
    "    plt.figure(figsize=(20, 1.5))\n",
    "    \n",
    "    # Get full, partial, or empty images\n",
    "    if capsid_type == 1:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 1]\n",
    "    elif capsid_type == 2:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 2]\n",
    "    elif capsid_type == 3:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 3]\n",
    "    elif capsid_type == 4:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 4]\n",
    "    elif capsid_type == 5:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 5]\n",
    "    elif capsid_type == 6:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 6]\n",
    "    else:\n",
    "        indices = [i for i in range(len(images)) if labels[i] == 7]\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        if i < len(indices):\n",
    "            index = indices[i]\n",
    "            plt.subplot(2, num_images // 2, i + 1)  # Two rows of images\n",
    "            plt.grid(False)\n",
    "            # Remove the axis\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(images[index], cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot 10 full capsids\n",
    "plot_images(train_images, train_labels, 50, 1)\n",
    "# Plot 10 partial capsids\n",
    "plot_images(train_images, train_labels, 50, 2)\n",
    "# Plot 10 empty capsids\n",
    "plot_images(train_images, train_labels, 50, 3)\n",
    "\n",
    "# Plot aggregation \n",
    "plot_images(train_images, train_labels, 50, 4)\n",
    "# Plot ice\n",
    "plot_images(train_images, train_labels, 50, 5)\n",
    "# Plot broken capsids\n",
    "plot_images(train_images, train_labels, 50, 6)\n",
    "# Plot background annotations\n",
    "plot_images(train_images, train_labels, 50, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full_images, partial_images, and empty_images list from the test_images list\n",
    "full_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 1]\n",
    "partial_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 2]\n",
    "empty_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 3]\n",
    "\n",
    "aggregation_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 4]\n",
    "ice_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 5]\n",
    "broken_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 6]\n",
    "background_images = [train_images[i] for i in range(len(train_images)) if train_labels[i] == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the length of the lists\n",
    "print(f\"There are {len(full_images)} full capsid images.\")\n",
    "print(f\"There are {len(partial_images)} partial capsid images.\")\n",
    "print(f\"There are {len(empty_images)} empty capsid images.\")\n",
    "\n",
    "print(f\"There are {len(aggregation_images)} aggregation images.\")\n",
    "print(f\"There are {len(ice_images)} ice images.\")\n",
    "print(f\"There are {len(broken_images)} broken images.\")\n",
    "print(f\"There are {len(background_images)} background images.\")\n",
    "\n",
    "# Create a bar chart of the number of images in each category\n",
    "\n",
    "# Create a list of the number of images in each category\n",
    "# num_images = [len(full_images), len(partial_images), len(empty_images)]\n",
    "num_images = [len(full_images), len(partial_images), len(empty_images), len(aggregation_images), len(ice_images), len(broken_images), len(background_images)]\n",
    "\n",
    "# Create a list of the labels\n",
    "# labels = ['Full', 'Partial', 'Empty']\n",
    "labels = ['Full', 'Partial', 'Empty', 'Aggregation', 'Ice', 'Broken', 'Background']\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(labels, num_images, color=['lightgreen', 'seagreen', 'skyblue'])\n",
    "plt.title('Number of Images in Each Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many instances of each class are in the testing and rw testing data\n",
    "print(f\"There are {(test_labels).count(1)} full capsid images in the testing data.\")\n",
    "print(f\"There are {(test_labels).count(2)} partial capsid images in the testing data.\")\n",
    "print(f\"There are {(test_labels).count(3)} empty capsid images in the testing data.\")\n",
    "\n",
    "print(f\"There are {(test_labels).count(4)} aggregation images in the testing data.\")\n",
    "print(f\"There are {(test_labels).count(5)} ice images in the testing data.\")\n",
    "print(f\"There are {(test_labels).count(6)} broken images in the testing data.\")\n",
    "print(f\"There are {(test_labels).count(7)} background images in the testing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are creating a plot that shows the Standard Deviation vs the Mean of each patch\n",
    "################################ capsids ####################################\n",
    "\n",
    "# convert the images to a numpy array\n",
    "empty_images = np.array(empty_images)\n",
    "full_images = np.array(full_images)\n",
    "partial_images = np.array(partial_images)\n",
    "\n",
    "# Take the mean and standard deviation of the images\n",
    "em = [x.mean() for x in empty_images]\n",
    "es = [x.std() for x in empty_images]\n",
    "\n",
    "fm = [x.mean() for x in full_images]\n",
    "fs = [x.std() for x in full_images]\n",
    "\n",
    "pm = [x.mean() for x in partial_images]\n",
    "ps = [x.std() for x in partial_images]\n",
    "\n",
    "################################ Debris ###################################\n",
    "\n",
    "am = [x.mean() for x in aggregation_images]\n",
    "as_ = [x.std() for x in aggregation_images]\n",
    "\n",
    "im = [x.mean() for x in ice_images]\n",
    "is_ = [x.std() for x in ice_images]\n",
    "\n",
    "bm = [x.mean() for x in broken_images]\n",
    "bs = [x.std() for x in broken_images]\n",
    "\n",
    "bam = [x.mean() for x in background_images]\n",
    "bas = [x.std() for x in background_images]\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# plot the mean and standard deviation of the empty capsids\n",
    "plt.plot(em, es, 'o', color='black', alpha=0.5)\n",
    "plt.plot(fm, fs, 'o', color='red', alpha=0.5)\n",
    "plt.plot(pm, ps, 'o', color='blue', alpha=0.5)\n",
    "\n",
    "plt.plot(am, as_, 'o', color='green', alpha=0.5)\n",
    "plt.plot(im, is_, 'o', color='purple', alpha=0.5)\n",
    "plt.plot(bm, bs, 'o', color='orange', alpha=0.5)\n",
    "plt.plot(bam, bas, 'o', color='yellow', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Standard Deviation')\n",
    "# plt.legend(['Empty', 'Full', 'Partial'])\n",
    "plt.legend(['Empty', 'Full', 'Partial', 'Aggregation', 'Ice', 'Broken', 'Background'])\n",
    "plt.title('Standard Deviation vs the Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## For Images With a Background ####################################\n",
    "\n",
    "# Create a Histogram of the average intensity values of the capsid patches\n",
    "plt.hist([x.mean() for x in empty_images], label='empty', alpha=0.2) \n",
    "plt.hist([x.mean() for x in full_images], label='full', alpha=0.2) \n",
    "plt.hist([x.mean() for x in partial_images], label='partial', alpha=0.2)\n",
    "\n",
    "plt.hist([x.mean() for x in aggregation_images], label='aggregation', alpha=0.2)\n",
    "plt.hist([x.mean() for x in ice_images], label='ice', alpha=0.2)\n",
    "plt.hist([x.mean() for x in broken_images], label='broken', alpha=0.2)\n",
    "plt.hist([x.mean() for x in background_images], label='background', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the images and get the center patch of the images\n",
    "left_center_coords = (empty_images[0].shape[0] // 3) # assuming a square and that all images are the same size\n",
    "right_center_coords = (2 * empty_images[0].shape[0] // 3)\n",
    "\n",
    "# Classifiying based on only using the inside of the capsid\n",
    "plt.hist([x[left_center_coords:right_center_coords, left_center_coords:right_center_coords].mean() for x in empty_images], label='empty', alpha=0.2) \n",
    "plt.hist([x[left_center_coords:right_center_coords, left_center_coords:right_center_coords].mean() for x in full_images], label='filled', alpha=0.2) \n",
    "plt.hist([x[left_center_coords:right_center_coords, left_center_coords:right_center_coords].mean() for x in partial_images], label='partial', alpha=0.2)\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Mean: Center Patch')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prototypical Capsids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will create the average protoypical full capsid by averaging the images in the synthetic\n",
    "# dataset. you will need to run those cells alone without the other real data\n",
    "# Then you could uncomment the following code to create the average images\n",
    "\n",
    "# # Convert the list of images to a numpy array\n",
    "# full_images_array = np.array(full_images)\n",
    "# partial_images_array = np.array(partial_images)\n",
    "# empty_images_array = np.array(empty_images)\n",
    "\n",
    "# # # Convert the images to float type for accurate averaging\n",
    "# full_images_float = full_images_array.astype(np.float32)\n",
    "# partial_images_float = partial_images_array.astype(np.float32)\n",
    "# empty_images_float = empty_images_array.astype(np.float32)\n",
    "\n",
    "# # Compute the average images\n",
    "# avg_filled = np.mean(full_images_float, axis=0)\n",
    "# avg_partial = np.mean(partial_images_float, axis=0)\n",
    "# avg_empty = np.mean(empty_images_float, axis=0)\n",
    "\n",
    "# # Clip the values to [0, 255] range and convert back to uint8 for display\n",
    "# avg_filled = np.clip(avg_filled, 0, 255).astype(np.uint8)\n",
    "# avg_partial = np.clip(avg_partial, 0, 255).astype(np.uint8)\n",
    "# avg_empty = np.clip(avg_empty, 0, 255).astype(np.uint8)\n",
    "\n",
    "# ################################### Save the Prototypical Patches ###################################\n",
    "\n",
    "# # Save the average images\n",
    "# cv2.imwrite(f\"{model_checkpoints}avg_filled.png\", avg_filled)\n",
    "# cv2.imwrite(f\"{model_checkpoints}avg_partial.png\", avg_partial)\n",
    "# cv2.imwrite(f\"{model_checkpoints}avg_empty.png\", avg_empty)\n",
    "\n",
    "################################### Load the Prototypical Patches ###################################\n",
    "\n",
    "# Load in the average images\n",
    "avg_filled = cv2.imread(f\"{model_checkpoints}avg_filled.png\")\n",
    "avg_partial = cv2.imread(f\"{model_checkpoints}avg_partial.png\")\n",
    "avg_empty = cv2.imread(f\"{model_checkpoints}avg_empty.png\")\n",
    "\n",
    "# resize the images to 224x224\n",
    "avg_filled = cv2.resize(avg_filled, (224, 224))\n",
    "avg_partial = cv2.resize(avg_partial, (224, 224))\n",
    "avg_empty = cv2.resize(avg_empty, (224, 224))\n",
    "\n",
    "######################################### Show the Images #########################################\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "# plot both of the avg_empty and the avg_filled on subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "#change the size of the figure\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "ax1.imshow(avg_filled, cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Average Filled Capsid')\n",
    "ax2.imshow(avg_partial, cmap='gray')    \n",
    "ax2.axis('off')\n",
    "ax2.set_title('Average Partial Capsid')\n",
    "ax3.imshow(avg_empty, cmap='gray')\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Average Empty Capsid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the average images\n",
    "print(f\"The shape of the average filled image is: {avg_filled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the similarity to the prototypical capsid to classify the capsids\n",
    "data = train_images\n",
    "labels  = train_labels\n",
    "f1 = [np.linalg.norm(x - avg_filled) for x in data] \n",
    "f2 = [np.linalg.norm(x - avg_partial) for x in data]\n",
    "f3 = [np.linalg.norm(x - avg_empty) for x in data]\n",
    "\n",
    "#change the opacity of the points\n",
    "plt.scatter(f1, f3, c=labels, alpha=0.2);\n",
    "\n",
    "# plt.xlabel('label')\n",
    "plt.xlabel('Distance From Average Filled Capsid')\n",
    "plt.ylabel('Distance From Average Empty Capsid')\n",
    "plt.title('Similarity to the Prototypical Capsid: Frobenius Norm of Pixelwise Difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Patches to Create Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper gives a good comparison of SIFT and HOG:\n",
    "\n",
    "Srinivas, D., & Hanumaji, K. (2019). Analysis of various image feature extraction methods against noisy image: SIFT, SURF and HOG. J Eng Sci, 10(2), 32-36."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Analysis Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## For Images With a Background ####################################\n",
    "\n",
    "# compute features on the patches and assesmble the features into a csv with the associated labels\n",
    "# features: mean, sd, median, mean of inner patch, sd of inner patch, median of inner patch, min, max, range, similarity to avg_filled, similarity to avg_partial, similarity to avg_empty\n",
    "# Do this for each of the list of images: full_images, partial_images, empty_images\n",
    "\n",
    "left_center_coords = (test_images[0].shape[0] // 3) # assuming a square and that all images are the same size\n",
    "right_center_coords = (2 * test_images[0].shape[0] // 3)\n",
    "\n",
    "avg_full_inner_patch = avg_filled[left_center_coords:right_center_coords, left_center_coords:right_center_coords]\n",
    "avg_partial_inner_patch = avg_partial[left_center_coords:right_center_coords, left_center_coords:right_center_coords]\n",
    "avg_empty_inner_patch = avg_empty[left_center_coords:right_center_coords, left_center_coords:right_center_coords]\n",
    "\n",
    "################################### Function to Compute Features #################################\n",
    "def compute_features(image_list, labels):\n",
    "    # initialize the lists\n",
    "    mean = []\n",
    "    sd = []\n",
    "    median = []\n",
    "    min_val = []\n",
    "    max_val = []\n",
    "    range_val = []\n",
    "    skewness = []\n",
    "    kurtosis_val = []\n",
    "    mode = []\n",
    "    mean_inner = []\n",
    "    sd_inner = []\n",
    "    median_inner = []\n",
    "    min_inner = []\n",
    "    max_inner = []\n",
    "    range_inner = []\n",
    "    skewness_inner = []\n",
    "    kurtosis_inner = []\n",
    "    mode_inner = []\n",
    "    sim_to_avg_full_inner = []\n",
    "    sim_to_avg_partial_inner = []\n",
    "    sim_to_avg_empty_inner = []\n",
    "    sim_to_avg_full = []\n",
    "    sim_to_avg_partial = []\n",
    "    sim_to_avg_empty = []\n",
    "    \n",
    "    # Iterate over the patches\n",
    "    for patch in image_list:\n",
    "        inner_patch = patch[left_center_coords:right_center_coords, left_center_coords:right_center_coords]\n",
    "        mean.append(np.mean(patch))\n",
    "        sd.append(np.std(patch))\n",
    "        median.append(np.median(patch))\n",
    "        min_val.append(np.min(patch))\n",
    "        max_val.append(np.max(patch))\n",
    "        range_val.append(np.max(patch) - np.min(patch))\n",
    "        skewness.append(skew(patch.flatten()))\n",
    "        kurtosis_val.append(kurtosis(patch.flatten()))\n",
    "        mode.append(np.argmax(np.bincount(patch.flatten())))\n",
    "        mean_inner.append(np.mean(inner_patch))\n",
    "        sd_inner.append(np.std(inner_patch))\n",
    "        median_inner.append(np.median(inner_patch))\n",
    "        min_inner.append(np.min(inner_patch))\n",
    "        max_inner.append(np.max(inner_patch))\n",
    "        range_inner.append(np.max(inner_patch) - np.min(inner_patch))\n",
    "        skewness_inner.append(skew(inner_patch.flatten()))\n",
    "        kurtosis_inner.append(kurtosis(inner_patch.flatten()))\n",
    "        mode_inner.append(np.argmax(np.bincount(inner_patch.flatten())))\n",
    "        sim_to_avg_full_inner.append(np.linalg.norm(inner_patch - avg_full_inner_patch))\n",
    "        sim_to_avg_partial_inner.append(np.linalg.norm(inner_patch - avg_partial_inner_patch))\n",
    "        sim_to_avg_empty_inner.append(np.linalg.norm(inner_patch - avg_empty_inner_patch))\n",
    "        sim_to_avg_full.append(np.linalg.norm(patch - avg_filled))\n",
    "        sim_to_avg_partial.append(np.linalg.norm(patch - avg_partial))\n",
    "        sim_to_avg_empty.append(np.linalg.norm(patch - avg_empty))\n",
    "        \n",
    "        \n",
    "    # normalize each of the statistics\n",
    "    mean = (mean - np.mean(mean)) / np.std(mean)\n",
    "    sd = (sd - np.mean(sd)) / np.std(sd)\n",
    "    median = (median - np.mean(median)) / np.std(median)\n",
    "    # min_val = (min_val - np.mean(min_val)) / np.std(min_val)\n",
    "    # max_val = (max_val - np.mean(max_val)) / np.std(max_val)\n",
    "    min_val = min_val / np.max(min_val)\n",
    "    max_val = max_val / np.max(max_val)\n",
    "    range_val = (range_val - np.mean(range_val)) / np.std(range_val)\n",
    "    skewness = (skewness - np.mean(skewness)) / np.std(skewness)\n",
    "    kurtosis_val = (kurtosis_val - np.mean(kurtosis_val)) / np.std(kurtosis_val)\n",
    "    mode = (mode - np.mean(mode)) / np.std(mode)\n",
    "    mean_inner = (mean_inner - np.mean(mean_inner)) / np.std(mean_inner)\n",
    "    sd_inner = (sd_inner - np.mean(sd_inner)) / np.std(sd_inner)\n",
    "    median_inner = (median_inner - np.mean(median_inner)) / np.std(median_inner)\n",
    "    min_inner = (min_inner - np.mean(min_inner)) / np.std(min_inner)\n",
    "    max_inner = (max_inner - np.mean(max_inner)) / np.std(max_inner)\n",
    "    range_inner = (range_inner - np.mean(range_inner)) / np.std(range_inner)\n",
    "    skewness_inner = (skewness_inner - np.mean(skewness_inner)) / np.std(skewness_inner)\n",
    "    kurtosis_inner = (kurtosis_inner - np.mean(kurtosis_inner)) / np.std(kurtosis_inner)\n",
    "    mode_inner = (mode_inner - np.mean(mode_inner)) / np.std(mode_inner)\n",
    "    sim_to_avg_full_inner = (sim_to_avg_full_inner - np.mean(sim_to_avg_full_inner)) / np.std(sim_to_avg_full_inner)\n",
    "    sim_to_avg_partial_inner = (sim_to_avg_partial_inner - np.mean(sim_to_avg_partial_inner)) / np.std(sim_to_avg_partial_inner)\n",
    "    sim_to_avg_empty_inner = (sim_to_avg_empty_inner - np.mean(sim_to_avg_empty_inner)) / np.std(sim_to_avg_empty_inner)\n",
    "    sim_to_avg_full = (sim_to_avg_full - np.mean(sim_to_avg_full)) / np.std(sim_to_avg_full)\n",
    "    sim_to_avg_partial = (sim_to_avg_partial - np.mean(sim_to_avg_partial)) / np.std(sim_to_avg_partial)\n",
    "    sim_to_avg_empty = (sim_to_avg_empty - np.mean(sim_to_avg_empty)) / np.std(sim_to_avg_empty)\n",
    "        \n",
    "    # Create a dataframe of the statistics\n",
    "    stats = pd.DataFrame({\n",
    "        'mean': mean,\n",
    "        'sd': sd,\n",
    "        'median': median,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'range': range_val,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurtosis_val,\n",
    "        'mode': mode,\n",
    "        'mean_inner': mean_inner,\n",
    "        'sd_inner': sd_inner,\n",
    "        'median_inner': median_inner,\n",
    "        'min_inner': min_inner,\n",
    "        'max_inner': max_inner,\n",
    "        'range_inner': range_inner,\n",
    "        'skewness_inner': skewness_inner,\n",
    "        'kurtosis_inner': kurtosis_inner,\n",
    "        'mode_inner': mode_inner,\n",
    "        'sim_to_avg_full_inner': sim_to_avg_full_inner,\n",
    "        'sim_to_avg_partial_inner': sim_to_avg_partial_inner,\n",
    "        'sim_to_avg_empty_inner': sim_to_avg_empty_inner,\n",
    "        'sim_to_avg_full': sim_to_avg_full,\n",
    "        'sim_to_avg_partial': sim_to_avg_partial,\n",
    "        'sim_to_avg_empty': sim_to_avg_empty\n",
    "    })\n",
    "    \n",
    "    # add the labels to the dataframe\n",
    "    stats['label'] = labels\n",
    "    \n",
    "    return stats\n",
    "\n",
    "################################# Compute Features for Training #######################################\n",
    "\n",
    "# Compute features for the training data\n",
    "train_img_analysis_features_df = compute_features(train_images, train_labels)\n",
    "\n",
    "# Compute features for the testing data\n",
    "test_img_analysis_features_df = compute_features(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Save the All Combined Data #######################################\n",
    "# save the df to a csv\n",
    "train_img_analysis_features_df.to_csv(f\"{split_data_folder}train_img_analysis_features_df.csv\", index=False)\n",
    "\n",
    "#save the df to a csv\n",
    "test_img_analysis_features_df.to_csv(f\"{split_data_folder}test_img_analysis_features_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the All Combined Capsid Dataset\n",
    "\n",
    "train_img_analysis_features_df = pd.read_csv(f\"{split_data_folder}train_img_analysis_features_df.csv\")\n",
    "test_img_analysis_features_df = pd.read_csv(f\"{split_data_folder}test_img_analysis_features_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the range, mean, and sd of each of the features\n",
    "feature_stats = test_img_analysis_features_df.describe()\n",
    "# print(feature_stats)\n",
    "\n",
    "# Create a table of the mean, sd, and range of each of the features\n",
    "feature_stats = feature_stats.loc[['mean', 'std', 'min', 'max']]\n",
    "\n",
    "# Convert feature_stats to a df\n",
    "feature_stats_df = pd.DataFrame(feature_stats)\n",
    "\n",
    "# show the feature_stats_df\n",
    "feature_stats_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will get the feature descriptors for one image using HOG\n",
    "\n",
    "Some of the following code comes from a tutorial at the following link:\n",
    "https://towardsdatascience.com/hog-histogram-of-oriented-gradients-67ecd887675f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hog_features = []\n",
    "for image in train_images:\n",
    "    fd, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
    "    train_hog_features.append(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also get the feautres for the test data\n",
    "test_hog_features = []\n",
    "for image in test_images:\n",
    "    fd, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
    "    test_hog_features.append(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train HOG features as a csv\n",
    "train_hog_features_df = pd.DataFrame(train_hog_features)\n",
    "train_hog_features_df['label'] = train_labels\n",
    "\n",
    "# Save the test HOG features as a csv\n",
    "test_hog_features_df = pd.DataFrame(test_hog_features)\n",
    "test_hog_features_df['label'] = test_labels\n",
    "\n",
    "############################## Save the features ########################################\n",
    "\n",
    "train_hog_features_df.to_csv(f\"{split_data_folder}train_hog_features_df.csv\", index=False)\n",
    "test_hog_features_df.to_csv(f\"{split_data_folder}test_hog_features_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the real data csvs\n",
    "train_hog_features_df = pd.read_csv(f\"{split_data_folder}train_hog_features_df.csv\")\n",
    "test_hog_features_df = pd.read_csv(f\"{split_data_folder}test_hog_features_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Fist we will create a library of visual words. This is done by getting the feature descriptors for all of the images, combining them into one np array, and clustering them down to 200 visual words. \n",
    "\n",
    "Some of the code in this section came from this blog post: https://liverungrow.medium.com/sift-bag-of-features-svm-for-classification-b5f775d8e55f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SIFT object\n",
    "sift = cv2.SIFT_create(contrastThreshold=0.01, edgeThreshold=30)\n",
    "sift_features = []\n",
    "sift_keypoints = []\n",
    "\n",
    "# Compute the SIFT features for each image\n",
    "for image in train_images:\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    sift_features.append(descriptors)\n",
    "    sift_keypoints.append(keypoints)\n",
    "\n",
    "# Concatenate the descriptors along the first axis\n",
    "sift_features_concat = np.concatenate(sift_features, axis=0)\n",
    "\n",
    "# Draw the keypoints on the first image\n",
    "img = np.array(train_images[5]).astype(np.uint8)  # Replace with the index of the image you want to visualize\n",
    "img_with_keypoints = cv2.drawKeypoints(img, sift_keypoints[0], img.copy())\n",
    "plt.imshow(img_with_keypoints)\n",
    "plt.show()\n",
    "\n",
    "# Now, sift_features_concat contains all descriptors in a single array\n",
    "print(sift_features_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Bag of Visual Words technique to be able to USE these SIFT features\n",
    "# This takes about 8 minutes to run\n",
    "\n",
    "print('Performing K-means clustering...')\n",
    "# Perform K-means clustering\n",
    "k = 200  # number of clusters/ visual words\n",
    "sift_kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "sift_kmeans.fit(sift_features_concat)\n",
    "print('K-means clustering complete.')\n",
    "\n",
    "# get the cluster centers\n",
    "cluster_centers = sift_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data cluster centers\n",
    "np.save(f\"{split_data_folder}sift_kmeans_cluster_centers.npy\", cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data cluster centers\n",
    "cluster_centers = np.load(f\"{split_data_folder}sift_kmeans_cluster_centers.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have our visual words we can generate the keypoints for every image and assign visual words to each image.\n",
    "\n",
    "This is done by generating the features for each of the images and assesing how close the keypoint is to each of the 200 vocab words. In this way we can determine how many of the 200 feature descriptors are actually present in each of the images. We can then build a histogram which compiles the number of times each cluster was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the images in the dataset, compute the distance to each of the cluster centers and compile the results into a histogram\n",
    "\n",
    "def compute_visual_words_histogram(cluster_centers, X):\n",
    "    # initialize the list of histograms\n",
    "    visual_words_histograms = []\n",
    "    \n",
    "    for image in X:\n",
    "        ####################### Compute the SIFT features #######################\n",
    "        # Compute the SIFT features\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None) # in sift it scales the images\n",
    "        \n",
    "        ############## Create a histogram of the cluster centers ################\n",
    "        \n",
    "        # Compute the euclidean distance from each descriptor to each cluster center\n",
    "        distances = np.linalg.norm(descriptors[:, None] - cluster_centers[None], axis=-1)\n",
    "        \n",
    "        # Get the bin assignment for each descriptor\n",
    "        bin_assignments = np.argmin(distances, axis=-1)\n",
    "        \n",
    "        # Classify each of the keypoints to a cluster center. Determine how many of visual words are in each image\n",
    "        visual_words = np.zeros(len(cluster_centers))\n",
    "        for bin_idx in bin_assignments:\n",
    "            visual_words[bin_idx] += 1\n",
    "        \n",
    "        # Normalize the histogram\n",
    "        visual_words = visual_words / np.sum(visual_words)\n",
    "        \n",
    "        # Append the histogram to the list of histograms\n",
    "        visual_words_histograms.append(visual_words)\n",
    "    \n",
    "    # Convert the list of histograms to a numpy array\n",
    "    visual_words_histograms = np.array(visual_words_histograms)\n",
    "    \n",
    "    # Convert the list of histograms to a df\n",
    "    visual_words_histograms_df = pd.DataFrame(visual_words_histograms)\n",
    "    \n",
    "    return visual_words_histograms_df, visual_words_histograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the visual words histograms for the training, testing, and real world testing data\n",
    "\n",
    "####################################### Training Data #######################################\n",
    "# Compute the visual words histograms for the training data\n",
    "train_sift_features_df, visual_words_histograms = compute_visual_words_histogram(cluster_centers, train_images)\n",
    "# Add the labels to the df\n",
    "train_sift_features_df['label'] = train_labels\n",
    "\n",
    "####################################### Testing Data ########################################\n",
    "# Compute the visual words histograms for the testing data\n",
    "test_sift_features_df, _    = compute_visual_words_histogram(cluster_centers, test_images)\n",
    "# Add the labels to the df\n",
    "test_sift_features_df['label'] = test_labels\n",
    "\n",
    "##################################### Save the real data #####################################\n",
    "train_sift_features_df.to_csv(f\"{split_data_folder}train_sift_features_df.csv\", index=False)\n",
    "test_sift_features_df.to_csv(f\"{split_data_folder}test_sift_features_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the real data csvs\n",
    "train_sift_features_df = pd.read_csv(f\"{split_data_folder}train_sift_features_df.csv\")\n",
    "test_sift_features_df = pd.read_csv(f\"{split_data_folder}test_sift_features_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the histogram of visual words for the first image\n",
    "plt.bar(np.arange(len(visual_words_histograms[0])), visual_words_histograms[0])\n",
    "plt.xlabel('Visual Word Index')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Visual Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CV Filter Banks Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a filter bank to create features. Each filter is one feature. Use max pooling so that each filter is only one feature.\n",
    "\n",
    "####################################### Filter Bank #######################################\n",
    "\n",
    "def sobel(image):\n",
    "    # Apply the Sobel filter to the image\n",
    "    filtered_image = cv2.Sobel(image, cv2.CV_64F, 1, 1)\n",
    "    # Return the filtered image\n",
    "    return filtered_image\n",
    "\n",
    "def laplacian(image):\n",
    "    # Apply the Laplacian filter to the image\n",
    "    filtered_image = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    # Return the filtered image\n",
    "    return filtered_image\n",
    "\n",
    "def gaussian_blur(image):\n",
    "    # Apply Gaussian blur to the image\n",
    "    blurred_image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "    # Return the blurred image\n",
    "    return blurred_image\n",
    "\n",
    "def prewitt(image):\n",
    "    # Apply the Prewitt filter to the image\n",
    "    filtered_image = cv2.filter2D(image, -1, np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]))\n",
    "    # Return the filtered image\n",
    "    return filtered_image\n",
    "\n",
    "def scharr(image):\n",
    "    # Apply the Scharr filter to the image\n",
    "    filtered_image = cv2.filter2D(image, -1, np.array([[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]]))\n",
    "    # Return the filtered image\n",
    "    return filtered_image\n",
    "\n",
    "def roberts(image):\n",
    "    # Apply the Roberts filter to the image\n",
    "    filtered_image = cv2.filter2D(image, -1, np.array([[-1, 0], [0, 1]]))\n",
    "    # Return the filtered image\n",
    "    return filtered_image\n",
    "\n",
    "def max_pool(image):\n",
    "    # Get the dimensions of the image\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Divide the image into nine sections\n",
    "    top_left = image[:height//3, :width//3]\n",
    "    top_center = image[:height//3, width//3:2*width//3]\n",
    "    top_right = image[:height//3, 2*width//3:]\n",
    "    \n",
    "    middle_left = image[height//3:2*height//3, :width//3]\n",
    "    middle_center = image[height//3:2*height//3, width//3:2*width//3]\n",
    "    middle_right = image[height//3:2*height//3, 2*width//3:]\n",
    "    \n",
    "    bottom_left = image[2*height//3:, :width//3]\n",
    "    bottom_center = image[2*height//3:, width//3:2*width//3]\n",
    "    bottom_right = image[2*height//3:, 2*width//3:]\n",
    "    \n",
    "    # Calculate the maximum value in each section\n",
    "    max_top_left = np.max(top_left)\n",
    "    max_top_center = np.max(top_center)\n",
    "    max_top_right = np.max(top_right)\n",
    "    \n",
    "    max_middle_left = np.max(middle_left)\n",
    "    max_middle_center = np.max(middle_center)\n",
    "    max_middle_right = np.max(middle_right)\n",
    "    \n",
    "    max_bottom_left = np.max(bottom_left)\n",
    "    max_bottom_center = np.max(bottom_center)\n",
    "    max_bottom_right = np.max(bottom_right)\n",
    "    \n",
    "    # Return the max values in each section as a list or array\n",
    "    return [max_top_left, max_top_center, max_top_right, \n",
    "            max_middle_left, max_middle_center, max_middle_right,\n",
    "            max_bottom_left, max_bottom_center, max_bottom_right]\n",
    "\n",
    "####################################### Extracting Features #######################################\n",
    "\n",
    "# filter_bank function\n",
    "def filter_bank(image):\n",
    "    # Create a list to hold the features\n",
    "    features_one_img = []\n",
    "    \n",
    "    # Create a list of filters\n",
    "    filters = [sobel, laplacian, gaussian_blur, prewitt, scharr, roberts]\n",
    "    \n",
    "    # Apply each filter to the image\n",
    "    for f in filters:\n",
    "        # Apply the filter to the image\n",
    "        filtered_image = f(image)\n",
    "        # Apply max pooling to the filtered image\n",
    "        pooled_values = max_pool(filtered_image)\n",
    "        # Append the pooled values to the features list\n",
    "        features_one_img.extend(pooled_values)\n",
    "    \n",
    "    # Return the features as a vector\n",
    "    return np.array(features_one_img)\n",
    "\n",
    "# here we will loop through the images and apply the filter bank to each image\n",
    "def cv_features(image_list):\n",
    "    # Create a list to hold the features\n",
    "    features_list = []\n",
    "    \n",
    "    # Loop through the images\n",
    "    for image in image_list:\n",
    "        # Apply the filter bank to the image\n",
    "        features = filter_bank(image)\n",
    "        # Append the features to the features list\n",
    "        features_list.append(features)\n",
    "        \n",
    "    # Return the features list\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Feature Extraction for the train data ############################\n",
    "\n",
    "# Apply the filter bank to the training images\n",
    "train_features = cv_features(train_images)\n",
    "\n",
    "# Create a df of the features\n",
    "train_features_df = pd.DataFrame(train_features, columns= ['sobel_tl', 'sobel_tc', 'sobel_tr', 'sobel_ml', 'sobel_mc', 'sobel_mr', 'sobel_bl', 'sobel_bc', 'sobel_br',\n",
    "                                                            'laplacian_tl', 'laplacian_tc', 'laplacian_tr', 'laplacian_ml', 'laplacian_mc', 'laplacian_mr', 'laplacian_bl', 'laplacian_bc', 'laplacian_br',\n",
    "                                                            'gaussian_blur_tl', 'gaussian_blur_tc', 'gaussian_blur_tr', 'gaussian_blur_ml', 'gaussian_blur_mc', 'gaussian_blur_mr', 'gaussian_blur_bl', 'gaussian_blur_bc', 'gaussian_blur_br',\n",
    "                                                            'prewitt_tl', 'prewitt_tc', 'prewitt_tr', 'prewitt_ml', 'prewitt_mc', 'prewitt_mr', 'prewitt_bl', 'prewitt_bc', 'prewitt_br',\n",
    "                                                            'scharr_tl', 'scharr_tc', 'scharr_tr', 'scharr_ml', 'scharr_mc', 'scharr_mr', 'scharr_bl', 'scharr_bc', 'scharr_br',\n",
    "                                                            'roberts_tl', 'roberts_tc', 'roberts_tr', 'roberts_ml', 'roberts_mc', 'roberts_mr', 'roberts_bl', 'roberts_bc', 'roberts_br'])\n",
    "                                                     \n",
    "\n",
    "# add the labels to the df\n",
    "train_features_df['label'] = train_labels\n",
    "\n",
    "############################## Features Extraction for the test data ############################\n",
    "\n",
    "# Apply the filter bank to the testing images\n",
    "test_features = cv_features(test_images)\n",
    "\n",
    "# Create a df of the features. Include the filter names as the column names\n",
    "test_features_df = pd.DataFrame(test_features, columns= ['sobel_tl', 'sobel_tc', 'sobel_tr', 'sobel_ml', 'sobel_mc', 'sobel_mr', 'sobel_bl', 'sobel_bc', 'sobel_br',\n",
    "                                                            'laplacian_tl', 'laplacian_tc', 'laplacian_tr', 'laplacian_ml', 'laplacian_mc', 'laplacian_mr', 'laplacian_bl', 'laplacian_bc', 'laplacian_br',\n",
    "                                                            'gaussian_blur_tl', 'gaussian_blur_tc', 'gaussian_blur_tr', 'gaussian_blur_ml', 'gaussian_blur_mc', 'gaussian_blur_mr', 'gaussian_blur_bl', 'gaussian_blur_bc', 'gaussian_blur_br',\n",
    "                                                            'prewitt_tl', 'prewitt_tc', 'prewitt_tr', 'prewitt_ml', 'prewitt_mc', 'prewitt_mr', 'prewitt_bl', 'prewitt_bc', 'prewitt_br',\n",
    "                                                            'scharr_tl', 'scharr_tc', 'scharr_tr', 'scharr_ml', 'scharr_mc', 'scharr_mr', 'scharr_bl', 'scharr_bc', 'scharr_br',\n",
    "                                                            'roberts_tl', 'roberts_tc', 'roberts_tr', 'roberts_ml', 'roberts_mc', 'roberts_mr', 'roberts_bl', 'roberts_bc', 'roberts_br'])\n",
    "\n",
    "# add the labels to the df\n",
    "test_features_df['label'] = test_labels\n",
    "\n",
    "######################################## Save the real data #######################################\n",
    "train_features_df.to_csv(f\"{split_data_folder}train_cv_features_df.csv\", index=False)\n",
    "test_features_df.to_csv(f\"{split_data_folder}test_cv_features_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the real data csvs\n",
    "train_cv_features_df = pd.read_csv(f\"{split_data_folder}train_cv_features_df.csv\")\n",
    "test_cv_features_df = pd.read_csv(f\"{split_data_folder}test_cv_features_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Image Anlaysis and Filter Bank Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the image analysis features with the CV features\n",
    "\n",
    "# first remove the labels from the dataframes\n",
    "train_cv_features_df_nolabels = train_cv_features_df.drop(columns=['label'])\n",
    "test_cv_features_df_nolabels = test_cv_features_df.drop(columns=['label'])\n",
    "train_img_analysis_features_df_nolabels = train_img_analysis_features_df.drop(columns=['label'])\n",
    "test_img_analysis_features_df_nolabels = test_img_analysis_features_df.drop(columns=['label'])\n",
    "\n",
    "# combine the dataframes\n",
    "train_combined_features_df = pd.concat([train_img_analysis_features_df_nolabels, train_cv_features_df_nolabels], axis=1)\n",
    "test_combined_features_df = pd.concat([test_img_analysis_features_df_nolabels, test_cv_features_df_nolabels], axis=1)\n",
    "\n",
    "# Add the labels back to the dataframes\n",
    "train_combined_features_df['label'] = train_img_analysis_features_df['label']\n",
    "test_combined_features_df['label'] = test_img_analysis_features_df['label']\n",
    "\n",
    "# Save the real data combined features to a csv\n",
    "train_combined_features_df.to_csv(f\"{split_data_folder}train_combined_features_df.csv\", index=False)\n",
    "test_combined_features_df.to_csv(f\"{split_data_folder}test_combined_features_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the real data csvs\n",
    "train_combined_features_df = pd.read_csv(f\"{split_data_folder}train_combined_features_df.csv\")\n",
    "test_combined_features_df = pd.read_csv(f\"{split_data_folder}test_combined_features_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Using the Features we created in the Previous Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we define functions for:\n",
    "\n",
    "0. Cross Validation Strategy\n",
    "1. SVM\n",
    "2. Decision Tree Classification\n",
    "3. Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is inspired by the following tutorial: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\n",
    "\n",
    "# Define custom scorers for precision and recall\n",
    "scoring = {\n",
    "    'precision': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "# First we will define our cross validation strategy\n",
    "def refit_strategy(cv_results):\n",
    "    \"\"\"Here we will define a function to get the parameters of the best estimator\n",
    "    First, we will filter by precision using a threshold, and then we will pick the best estimator of the remaining based on the recall\n",
    "    \"\"\"\n",
    "    precision_threshold = 0.60\n",
    "    \n",
    "    cv_results_ = pd.DataFrame(cv_results)\n",
    "    \n",
    "    # Filter by precision\n",
    "    high_precision_cv_results = cv_results_[cv_results_['mean_test_precision'] > precision_threshold]\n",
    "    \n",
    "    if not high_precision_cv_results.empty:\n",
    "        best_recall_index = high_precision_cv_results['mean_test_recall'].idxmax()\n",
    "        return best_recall_index\n",
    "    else:\n",
    "        # if the precision is too low, we will just pick the best estimator based on the recall\n",
    "        best_recall_index = cv_results_['mean_test_recall'].idxmax()\n",
    "        return best_recall_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that uses a SVM and takes in training and \n",
    "# testing data and returns the accuracy and kappa score and plots the confusion matrix\n",
    "def svm_classifier(train_features_df, test_features_df, model_checkpoints, model_name):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train = train_features_df.drop(columns=['label'])\n",
    "    y_train = train_features_df['label']\n",
    "    \n",
    "    X_test = test_features_df.drop(columns=['label'])\n",
    "    y_test = test_features_df['label']\n",
    "    \n",
    "    # ############################## Grid Search ##############################\n",
    "    # # tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 0.01, 0.2, 0.5],\n",
    "    # #                     'C': [1, 10, 100, 1000]},\n",
    "    # #                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "    \n",
    "    # model = GridSearchCV(SVC(), tuned_parameters, scoring=scoring, refit=refit_strategy)\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    ############################# Random Search ##############################\n",
    "    # specify parameters and distributions to sample from\n",
    "    parameter_distribution = {'kernel': ['rbf', 'linear'], 'C': stats.uniform(1, 1000), 'gamma': stats.uniform(0.0001, 0.1)}\n",
    "    \n",
    "    n_iter_search = 20\n",
    "    model = RandomizedSearchCV(SVC(), param_distributions=parameter_distribution, n_iter=n_iter_search, scoring=scoring, refit=refit_strategy)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    ############################## Save the Model ##############################\n",
    "    # this tutorial was used for saving the model: \n",
    "    # https://saturncloud.io/blog/sklearn-how-to-save-a-model-created-from-a-pipeline-and-gridsearchcv-using-joblib-or-pickle/#:~:text=To%20save%20a%20model%20using%20Pickle%2C%20you%20need%20to%20import,model%20and%20the%20file%20name.&text=(model%2C%20f)-,To%20load%20the%20saved%20model%2C%20you%20need%20to%20import%20the,function%20with%20the%20file%20name.\n",
    "\n",
    "    # join the model name with the model_save_path\n",
    "    model_save_path = f\"{model_checkpoints}{model_name}.pkl\"\n",
    "\n",
    "    # save the model \n",
    "    joblib.dump(model, model_save_path)\n",
    "    \n",
    "    # # Load in the saved model\n",
    "    # model = joblib.load(model_save_path)\n",
    "    \n",
    "    ###########################################################################\n",
    "    \n",
    "    print(f\"Best Estimator: {model.best_estimator_}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"The accuracy of the SVM classifier is {accuracy}\")\n",
    "\n",
    "    # Compute the kappa score\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    print(f\"The kappa score of the SVM classifier is {kappa}\")\n",
    "    \n",
    "    # Compute the sensitivity for empty capsids\n",
    "    empty_sensitivity = confusion_matrix(y_test, y_pred)[2, 2] / np.sum(confusion_matrix(y_test, y_pred)[2])\n",
    "    print(f\"The sensitivity for empty capsids is {empty_sensitivity}\")\n",
    "\n",
    "    # Binarize the true labels\n",
    "    # y_test_bin = label_binarize(y_test, classes=[1, 2, 3])\n",
    "    y_test_bin = label_binarize(y_test, classes=[1, 2, 3, 4, 5, 6, 7])\n",
    "    # Predict probabilities for each class\n",
    "    y_pred_prob = model.decision_function(X_test)\n",
    "    # Calculate ROC-AUC scores for each class\n",
    "    auroc = roc_auc_score(y_test_bin, y_pred_prob, multi_class='ovr')\n",
    "    print(f\"The AUROC of the SVM classifier is {auroc}\")\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['Full', 'Partial', 'Empty'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['Full', 'Partial', 'Empty', 'Agg', 'Ice', 'Broken', 'Backgrd'])\n",
    "    disp.plot(cmap = 'Blues')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that uses a Decision Tree Classifier and takes in training and\n",
    "# testing data and returns the accuracy and kappa score and plots the confusion matrix\n",
    "def decision_tree_classifier(train_features_df, test_features_df, model_checkpoints, model_name, print_tree = False):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train = train_features_df.drop(columns=['label'])\n",
    "    y_train = train_features_df['label']\n",
    "    \n",
    "    X_test = test_features_df.drop(columns=['label'])\n",
    "    y_test = test_features_df['label']\n",
    "    \n",
    "    ############################## Random Search ##############################\n",
    "    # specify parameters and distributions to sample from\n",
    "    parameter_distribution = {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': stats.randint(1, 100),\n",
    "        'min_samples_split': stats.randint(2, 10),\n",
    "        'min_samples_leaf': stats.randint(1, 10),\n",
    "        'max_features': stats.randint(1, 10)\n",
    "    }\n",
    "    \n",
    "    n_iter_search = 200\n",
    "    model = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=parameter_distribution, n_iter=n_iter_search, scoring=scoring, refit=refit_strategy)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    ################################# Save the Model ##################################\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f\"{model_checkpoints}{model_name}.pkl\"\n",
    "    joblib.dump(model, model_save_path)\n",
    "    \n",
    "    # Load in the saved model\n",
    "    # model = joblib.load(model_save_path)\n",
    "    \n",
    "    ####################### Make Predictions on the Test Data ########################\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"Best Estimator: {model.best_estimator_}\")\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"The accuracy of the decision tree classifier is {accuracy}\")\n",
    "\n",
    "    # Compute the kappa score\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    print(f\"The kappa score of the decision tree classifier is {kappa}\")\n",
    "    \n",
    "    # Compute the sensitivity for empty capsids\n",
    "    empty_sensitivity = confusion_matrix(y_test, y_pred)[2, 2] / np.sum(confusion_matrix(y_test, y_pred)[2])\n",
    "    print(f\"The sensitivity for empty capsids is {empty_sensitivity}\")\n",
    "    \n",
    "    # Binarize the true labels\n",
    "    # y_test_bin = label_binarize(y_test, classes=[1, 2, 3])\n",
    "    y_test_bin = label_binarize(y_test, classes=[1, 2, 3, 4, 5, 6, 7])\n",
    "    # Predict probabilities for each class\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    # Calculate ROC-AUC scores for each class\n",
    "    auroc = roc_auc_score(y_test_bin, y_pred_prob, multi_class='ovr')\n",
    "    print(f\"The AUROC of the decision tree classifier is {auroc}\")\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['Full', 'Partial', 'Empty'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['Full', 'Partial', 'Empty', 'Agg', 'Ice', 'Broken', 'Backgrd'])\n",
    "    disp.plot(cmap = 'Blues')\n",
    "    plt.show()\n",
    "    \n",
    "    if print_tree:\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        # tree.plot_tree(model, filled=True, feature_names=X_train.columns, class_names=['Full', 'Partial', 'Empty'])\n",
    "        tree.plot_tree(model, filled=True, feature_names=X_train.columns, class_names=['Full', 'Partial', 'Empty', 'Agg', 'Ice', 'Broken', 'Backgrd'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that uses a Kmeans Clustering and takes in training and\n",
    "# testing data and returns the accuracy and kappa score and plots the confusion matrix\n",
    "def kmeans_clustering(train_features_df, test_features_df, model_checkpoints, model_name):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train = train_features_df.drop(columns=['label'])\n",
    "    y_train = train_features_df['label']\n",
    "    \n",
    "    X_test = test_features_df.drop(columns=['label'])\n",
    "    y_test = test_features_df['label']\n",
    "    \n",
    "    # ############################## Grid Search ##############################\n",
    "    # specify parameters\n",
    "    # tuned_parameters = [{'n_clusters': [3], 'init': ['k-means++', 'random'], 'algorithm': ['lloyd', 'elkan'], 'n_init': [10, 20]}] # change this line if you change the number of classes. num classes = num clusters\n",
    "    # tuned_parameters = [{'n_clusters': [7], 'init': ['k-means++', 'random'], 'algorithm': ['lloyd', 'elkan'], 'n_init': [10, 20]}] # change this line if you change the number of classes. num classes = num clusters\n",
    "\n",
    "    ################################ Random Search ##############################\n",
    "    # specify parameters and distributions to sample from\n",
    "    parameter_distribution = {\n",
    "        'n_clusters': [3], # change this line if you change the number of classes. num classes = num clusters\n",
    "        # 'n_clusters': [7], # change this line if you change the number of classes. num classes = num clusters\n",
    "        'init': ['k-means++', 'random'],\n",
    "        'algorithm': ['lloyd', 'elkan'],\n",
    "        'n_init': stats.randint(10, 20)\n",
    "    }\n",
    "    n_iter_search = 20\n",
    "    model = RandomizedSearchCV(KMeans(), param_distributions=parameter_distribution, n_iter=n_iter_search, scoring=scoring, refit=refit_strategy, verbose=3)\n",
    "    \n",
    "    # model = GridSearchCV(KMeans(), tuned_parameters, scoring=scoring, refit=refit_strategy)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Estimator: {model.best_estimator_}\")\n",
    "\n",
    "    ################################# Save the Model ##################################\n",
    "\n",
    "    # save the model\n",
    "    model_save_path = f\"{model_checkpoints}{model_name}.pkl\"\n",
    "    joblib.dump(model, model_save_path)\n",
    "    \n",
    "    # Load in the saved model\n",
    "    model = joblib.load(model_save_path)\n",
    "    \n",
    "    ########################### Assigning Each Cluster ###############################\n",
    "\n",
    "    # Get the cluster assignments\n",
    "    cluster_assignments = model.predict(X_train)+1\n",
    "\n",
    "    # Create a df of the cluster assignments\n",
    "    cluster_assignments_df = pd.DataFrame({'cluster': cluster_assignments, 'label': y_train})\n",
    "\n",
    "    # For each cluster, find the majority vote label\n",
    "    majority_vote_labels = cluster_assignments_df.groupby('cluster')['label'].agg(lambda x: np.bincount(x).argmax())\n",
    "\n",
    "    # Create a mapping dictionary from cluster to majority vote label\n",
    "    cluster_label_mapping = dict(zip(majority_vote_labels.index, majority_vote_labels.values))\n",
    "\n",
    "    # Map the cluster assignments to the majority vote labels\n",
    "    cluster_assignments_df['majority_label'] = cluster_assignments_df['cluster'].map(cluster_label_mapping)\n",
    "\n",
    "    ################################# Save the Model ##################################\n",
    "    \n",
    "    # save the cluster label mapping\n",
    "    cluster_label_mapping_save_path = f\"{model_checkpoints}{model_name}_cluster_label_mapping.pkl\"\n",
    "    joblib.dump(cluster_label_mapping, cluster_label_mapping_save_path)\n",
    "    \n",
    "    # Load in the saved model\n",
    "    # cluster_label_mapping = joblib.load(cluster_label_mapping_save_path)\n",
    "    \n",
    "    ####################### Make Predictions on the Test Data ########################\n",
    "    \n",
    "    # Get the cluster assignments for the test data\n",
    "    cluster_assignments_test = model.predict(X_test)+1\n",
    "    \n",
    "    # Map the cluster assignments to the majority vote labels\n",
    "    cluster_assignments_test_df = pd.DataFrame({'cluster': cluster_assignments_test, 'label': y_test})\n",
    "    cluster_assignments_test_df['majority_label'] = cluster_assignments_test_df['cluster'].map(cluster_label_mapping)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = cluster_assignments_test_df['majority_label']\n",
    "    \n",
    "    ############################# Calculate the Metrics ##############################\n",
    "\n",
    "    # Calculate the accuracy of the clustering\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"The accuracy of the clustering is {accuracy}\")\n",
    "\n",
    "    # Calculate the kappa score of the clustering\n",
    "    kappa_cluster = cohen_kappa_score(y_test, y_pred)\n",
    "    print(f\"The kappa score of the clustering is {kappa_cluster}\")\n",
    "    \n",
    "    # Compute the sensitivity for empty capsids\n",
    "    empty_sensitivity = confusion_matrix(y_test, y_pred)[2, 2] / np.sum(confusion_matrix(cluster_assignments_test_df['label'], cluster_assignments_test_df['majority_label'])[2])\n",
    "    print(f\"The sensitivity for empty capsids is {empty_sensitivity}\")\n",
    "    \n",
    "    # Calculate the AUROC of the clustering\n",
    "    # y_test_bin = label_binarize(y_test, classes=[1, 2, 3])\n",
    "    # y_pred_bin = label_binarize(y_pred, classes=[1, 2, 3])\n",
    "    y_test_bin = label_binarize(y_test, classes=[1, 2, 3, 4, 5, 6, 7])\n",
    "    y_pred_bin = label_binarize(y_pred, classes=[1, 2, 3, 4, 5, 6, 7])\n",
    "    auroc = roc_auc_score(y_test_bin, y_pred_bin, multi_class='ovr')\n",
    "    print(f\"The AUROC of the clustering is {auroc}\")\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm_cluster_assignments = confusion_matrix(y_test, y_pred)\n",
    "    # disp_cluster_assignments = ConfusionMatrixDisplay(confusion_matrix=cm_cluster_assignments, display_labels = ['Full', 'Partial', 'Empty'])\n",
    "    disp_cluster_assignments = ConfusionMatrixDisplay(confusion_matrix=cm_cluster_assignments, display_labels = ['Full', 'Partial', 'Empty', 'Agg', 'Ice', 'Broken', 'Backgrd'])\n",
    "    \n",
    "    disp_cluster_assignments.plot(cmap = 'Blues')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image analysis ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ SVM Classifier img_an ############################\n",
    "\n",
    "svm_classifier(train_img_analysis_features_df, test_img_analysis_features_df, model_checkpoints, model_name=\"svm_model_img_an\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Decision Tree Classifier img_an ############################\n",
    "\n",
    "decision_tree_classifier(train_img_analysis_features_df, test_img_analysis_features_df, model_checkpoints, model_name=\"decision_tree_model_img_an\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Kmeans Clustering img_an ############################\n",
    "\n",
    "kmeans_clustering(train_img_analysis_features_df, test_img_analysis_features_df, model_checkpoints, model_name=\"kmeans_model_img_an\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## SVM Classifier HOG ############################\n",
    "\n",
    "svm_classifier(train_hog_features_df, test_hog_features_df, model_checkpoints, model_name=\"svm_model_hog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Decision Tree Classifier HOG ############################\n",
    "\n",
    "decision_tree_classifier(train_hog_features_df, test_hog_features_df, model_checkpoints, model_name=\"decision_tree_model_hog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Kmeans Clustering HOG ############################\n",
    "\n",
    "kmeans_clustering(train_hog_features_df, test_hog_features_df, model_checkpoints, model_name=\"kmeans_model_hog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT ML models\n",
    "\n",
    "Here is the original paper that made this method:\n",
    "\n",
    "Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60, 91-110."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ SVM Classifier sift ############################\n",
    "\n",
    "svm_classifier(train_sift_features_df, test_sift_features_df, model_checkpoints, model_name=\"svm_model_sift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Decision Tree Classifier sift ############################\n",
    "\n",
    "decision_tree_classifier(train_sift_features_df, test_sift_features_df, model_checkpoints, model_name=\"decision_tree_model_sift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Kmeans Clustering sift ############################\n",
    "\n",
    "kmeans_clustering(train_sift_features_df, test_sift_features_df, model_checkpoints, model_name=\"kmeans_model_sift\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Filter Bank ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ SVM Classifier cv ############################\n",
    "\n",
    "svm_classifier(train_cv_features_df, test_cv_features_df, model_checkpoints, model_name=\"svm_model_cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Decision Tree Classifier cv ############################\n",
    "\n",
    "decision_tree_classifier(train_cv_features_df, test_cv_features_df, model_checkpoints, model_name=\"decision_tree_model_cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Kmeans Clustering cv ############################\n",
    "\n",
    "kmeans_clustering(train_cv_features_df, test_cv_features_df, model_checkpoints, model_name=\"kmeans_model_cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Features ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ SVM Classifier combined ############################\n",
    "\n",
    "svm_classifier(train_combined_features_df, test_combined_features_df, model_checkpoints, model_name=\"svm_model_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Decision Tree Classifier combined ############################\n",
    "\n",
    "decision_tree_classifier(train_combined_features_df, test_combined_features_df, model_checkpoints, model_name=\"decision_tree_model_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Kmeans Clustering combined ############################\n",
    "\n",
    "kmeans_clustering(train_combined_features_df, test_combined_features_df, model_checkpoints, model_name=\"kmeans_model_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capsidize1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
