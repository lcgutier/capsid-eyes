{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to:\n",
    "1) Create a balanced dataset\n",
    "2) Split the data into training/validation, development, and testing\n",
    "3) Save the data to the split cryo data folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will create patches from the annotated coco data \n",
    "1) Load in the coco annotations and labels\n",
    "2) load in the images that are labeled\n",
    "3) create patches from the images using the coco labels and save to a new file\n",
    "4) Create a new csv of the images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the coco annotations\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# For Creating a split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Save and Load Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the path to all of the json files that contain the annotations:\n",
    "path_to_annotations = \"your_path_here/JSON_annotations/\"\n",
    "\n",
    "image_folder_data5 = \"your_path_here/CLAHE_corrected_data/\"\n",
    "\n",
    "image_folder_data = [image_folder_data5]\n",
    "# Note: if you have multiple folders, you can add them to the list above\n",
    "\n",
    "patches_image_directory = \"your_path_here/All_Patches/\"\n",
    "\n",
    "cryo_data_folder = \"your_path_here/Cryo_Data/\" # Data sent here is not split into train, test, develop\n",
    "combined_split_data = \"your_path_here/Train_test_develop_split_combined_background/\"\n",
    "\n",
    "synthetic_data = \"your_path_here/Synthetic_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Image Patches\n",
    "\n",
    "Loop Through COCO Formated Cryo Annotations and Get Patch Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decode_fast function is referenced from the following link:\n",
    "# https://gist.github.com/akTwelve/dc0bbbf26fb14493898fc74cd2aa7f74\n",
    "\n",
    "def decode_fast(mask_rle, shape):\n",
    "    # get the width and height of the image\n",
    "    height, width = shape\n",
    "    zero_one = np.zeros_like(mask_rle, dtype=np.uint8)\n",
    "    zero_one[1::2] = 255\n",
    "    zero_one = zero_one.reshape((len(mask_rle), 1))\n",
    "    expanded = np.repeat(zero_one, mask_rle, axis=0)\n",
    "    filled = np.append(expanded, np.zeros(width * height - len(expanded)))\n",
    "    im_arr = filled.reshape((height, width), order='F').astype(np.uint8)    \n",
    "    return im_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_5 = \"100F_All_Labels\"\n",
    "image_sets = [image_set_5]\n",
    "\n",
    "datasets = []\n",
    "\n",
    "# Load in the JSON files. Create a for loop that loads in the json files \n",
    "for i, val in enumerate(image_sets):\n",
    "    with open(path_to_annotations + val + \".json\") as f:\n",
    "        globals()['data' + str(i+1)] = json.load(f) # creates a global variable for each data set\n",
    "        # append the data to the datasets list\n",
    "        datasets.append(globals()['data' + str(i+1)])\n",
    "\n",
    "# Iterate over annotations for all images in all of the datasets\n",
    "\n",
    "# Choose a background type. True for keeping the background, False for removing the background\n",
    "background = True\n",
    "\n",
    "# initialize lists to store patch names and labels\n",
    "patch_names = []\n",
    "labels = []\n",
    "\n",
    "for i, data in enumerate(datasets):\n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        image_info = next((img for img in data['images'] if img['id'] == image_id), None) #takes only the image info for the image id of that annotation\n",
    "\n",
    "        if image_info:\n",
    "            image_file = f\"{image_folder_data[i]}{image_info['file_name']}\"\n",
    "            original_image = cv2.imread(image_file)\n",
    "\n",
    "            ########################## Create Patches Using BBOX ###########################\n",
    "            if background == True: # if true, here we keep the background\n",
    "                bbox = annotation['bbox']\n",
    "                x, y, width, height = map(int, bbox)\n",
    "                patch = original_image[y:y+height, x:x+width]\n",
    "\n",
    "            ##################### Create Patches Using Segmentation ########################\n",
    "            else: # if false, here we remove the background \n",
    "                counts_size = annotation['segmentation']\n",
    "                mask_rle = annotation['segmentation']['counts']\n",
    "                \n",
    "                # Convert the mask_rle to a numpy array\n",
    "                mask_rle = np.array(mask_rle)\n",
    "                \n",
    "                mask = decode_fast(mask_rle, (image_info['height'], image_info['width']))\n",
    "                \n",
    "                # Apply the mask to the original image\n",
    "                patch = cv2.bitwise_and(original_image, original_image, mask=mask)\n",
    "                \n",
    "                # Use the bbox to crop the patch\n",
    "                bbox = annotation['bbox']\n",
    "                x, y, width, height = map(int, bbox)\n",
    "                patch = patch[y:y+height, x:x+width]\n",
    "            \n",
    "            # get the label of the annotation\n",
    "            label = annotation['category_id']\n",
    "            labels.append(label)\n",
    "            \n",
    "            # Save the patch\n",
    "            patch_name = f\"{image_sets[i]}_{image_id}_{annotation['id']}.png\"\n",
    "            patch_filename = f\"{patches_image_directory}{patch_name}\"\n",
    "            \n",
    "            # save the patch name to a list of patch names\n",
    "            patch_names.append(patch_name)\n",
    "            \n",
    "            # Save the patch\n",
    "            cv2.imwrite(patch_filename, patch)\n",
    "            \n",
    "    # When done with the annotations for one dataset, print a message\n",
    "    print(f\"Finished processing {image_sets[i]}\")\n",
    "            \n",
    "# Create a df of the patch names and labels\n",
    "names_labels_df = pd.DataFrame({'patch_name': patch_names, 'label': labels})\n",
    "\n",
    "# # Save the df as a csv \n",
    "names_labels_df.to_csv(f\"{combined_split_data}names_labels_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in each of the patches one at a time from All_Patches and resize them to 224x224. Use the names_labels_df to get the names\n",
    "# This code was added after I had already created patches to make classification easier. You could also add it above :)\n",
    "# Image files that are too large have to be loaded this way to avoid memory issues\n",
    "\n",
    "# Create a new directory for the resized patches\n",
    "resized_patches_directory = \"your_path_here/All_Patches_resized/\"\n",
    "names_labels_df = pd.read_csv(f\"{combined_split_data}names_labels_df.csv\")\n",
    "\n",
    "# Iterate over the patch names and labels\n",
    "for index, row in names_labels_df.iterrows():\n",
    "    patch_name = row['patch_name']\n",
    "    \n",
    "    # Load the patch\n",
    "    patch_file = f\"{patches_image_directory}{patch_name}\"\n",
    "    patch = cv2.imread(patch_file)\n",
    "    \n",
    "    # Resize the patch to 224x224\n",
    "    resized_patch = cv2.resize(patch, (224, 224))\n",
    "    \n",
    "    # Save the resized patch\n",
    "    resized_patch_filename = f\"{resized_patches_directory}{patch_name}\"\n",
    "    cv2.imwrite(resized_patch_filename, resized_patch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load in the names_labels_df csv and turn it into a df\n",
    "names_labels_df = pd.read_csv(f\"{combined_split_data}names_labels_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many patches are full partial and empty in each dataset\n",
    "# Use names_labels_df to get the counts of each label in each dataset\n",
    "for i, val in enumerate(image_sets):\n",
    "    dataset = names_labels_df[names_labels_df['patch_name'].str.contains(val)]\n",
    "    print(f\"Dataset: {val}\")\n",
    "    print(dataset['label'].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ categorize the patches into their prelabeled class ############################\n",
    "\n",
    "# sort the patches into full and empty based on their labels using the df and the patches_image_directory.\n",
    "\n",
    "# Create an evenly split dataset of full, partial, and empty capsids.  \n",
    "# Note: Where: \"categories\":[{\"id\":1,\"name\":\"Full\",\"supercategory\":\"\"},{\"id\":2,\"name\":\"Partial\",\"supercategory\":\"\"},{\"id\":3,\"name\":\"Empty\",\"supercategory\":\"\"}]\n",
    "\n",
    "full_names = []\n",
    "partial_names = []\n",
    "empty_names = []\n",
    "\n",
    "full_labels = []\n",
    "partial_labels = []\n",
    "empty_labels = []\n",
    "\n",
    "####### This section is if there is debris ########\n",
    "aggregation_names = []\n",
    "ice_names = []\n",
    "broken_names = []\n",
    "background_names = []\n",
    "\n",
    "aggregation_labels = []\n",
    "ice_labels = []\n",
    "broken_labels = []\n",
    "background_labels = []\n",
    "\n",
    "for _, row in names_labels_df.iterrows():\n",
    "    if row['label'] == 1:\n",
    "        # append the image name and label to the lists\n",
    "        full_names.append(row['patch_name'])\n",
    "        full_labels.append(row['label'])\n",
    "        \n",
    "    elif row['label'] == 2:\n",
    "        # append the image name and label to the lists\n",
    "        partial_names.append(row['patch_name'])\n",
    "        partial_labels.append(row['label'])\n",
    "    # else:\n",
    "    elif row['label'] == 3:\n",
    "        # append the image name and label to the lists\n",
    "        empty_names.append(row['patch_name'])\n",
    "        empty_labels.append(row['label'])\n",
    "        \n",
    "    ############################# This section is if there is debris in the dataset #######################\n",
    "    elif row['label'] == 4:\n",
    "        # append the image name and label to the lists\n",
    "        aggregation_names.append(row['patch_name'])\n",
    "        aggregation_labels.append(row['label'])\n",
    "        \n",
    "    elif row['label'] == 5:\n",
    "        # append the image name and label to the lists\n",
    "        ice_names.append(row['patch_name'])\n",
    "        ice_labels.append(row['label'])\n",
    "        \n",
    "    elif row['label'] == 6:\n",
    "        # append the image name and label to the lists\n",
    "        broken_names.append(row['patch_name'])\n",
    "        broken_labels.append(row['label'])\n",
    "        \n",
    "    else:\n",
    "        background_names.append(row['patch_name'])\n",
    "        background_labels.append(row['label'])\n",
    "    ######################################################################################################\n",
    "\n",
    "# Find the minimum number of patches for each class\n",
    "min_num = min(len(full_names), len(partial_names), len(empty_names), len(aggregation_names), len(ice_names), len(broken_names), len(background_names))\n",
    "\n",
    "################# Create a new list of file names and labels that are evenly split but make it random #################\n",
    "\n",
    "random.seed(0)\n",
    "full_names = random.sample(full_names, min_num)    \n",
    "partial_names = random.sample(partial_names, min_num)\n",
    "empty_names = random.sample(empty_names, min_num)\n",
    "\n",
    "aggregation_names = random.sample(aggregation_names, min_num)\n",
    "ice_names = random.sample(ice_names, min_num)\n",
    "broken_names = random.sample(broken_names, min_num)\n",
    "background_names = random.sample(background_names, min_num)\n",
    "\n",
    "full_labels = [1]*min_num \n",
    "partial_labels = [2]*min_num\n",
    "empty_labels = [3]*min_num\n",
    "\n",
    "aggregation_labels = [4]*min_num\n",
    "ice_labels = [5]*min_num\n",
    "broken_labels = [6]*min_num\n",
    "background_labels = [7]*min_num\n",
    "\n",
    "# Print the length of the lists\n",
    "print(f\"Full: {len(full_names)}\")\n",
    "print(f\"Partial: {len(partial_names)}\")\n",
    "print(f\"Empty: {len(empty_names)}\")\n",
    "print(f\"Aggregation: {len(aggregation_names)}\")\n",
    "print(f\"Ice: {len(ice_names)}\")\n",
    "print(f\"Broken: {len(broken_names)}\")\n",
    "print(f\"Background: {len(background_names)}\")\n",
    "\n",
    "# append the full, partial, and empty lists together\n",
    "even_file_names = full_names + partial_names + empty_names + aggregation_names + ice_names + broken_names + background_names\n",
    "even_labels = full_labels + partial_labels + empty_labels + aggregation_labels + ice_labels + broken_labels + background_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the Dataset into Training, Development, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an even split of the classes using even_file_names and even_labels\n",
    "\n",
    "############################# Split the data into two partitions ############################\n",
    "tra_val_file_names, dev_file_names, tra_val_labels, dev_labels = train_test_split(even_file_names, even_labels, test_size=0.2, random_state=42, stratify=even_labels)\n",
    "\n",
    "######################### Save the file names and labels to a csv ############################\n",
    "# create a df with the file names and labels\n",
    "tra_val_df = pd.DataFrame({'patch_name': tra_val_file_names, 'label': tra_val_labels})\n",
    "dev_df     = pd.DataFrame({'patch_name': dev_file_names, 'label': dev_labels})\n",
    "\n",
    "# Save the df as a csv\n",
    "tra_val_df.to_csv(f\"{combined_split_data}tra_val_names_labels_df.csv\", index=False)\n",
    "dev_df.to_csv(f\"{combined_split_data}dev_names_labels_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Load in the synthetic data ###################################\n",
    "\n",
    "# Load in capsids.npy and labels.npy from the synthetic data\n",
    "synthetic_images = np.load(f\"{synthetic_data}capsids.npy\")\n",
    "synthetic_labels = np.load(f\"{synthetic_data}labels.npy\")\n",
    "\n",
    "################### Split the Synthetic data into three partitions ##########################\n",
    "# Create three partitions of the data: 20% development, 70% training and cross-validation, 10% final real world test data\n",
    "\n",
    "# Use stratified random sampling to split the data into three partitions\n",
    "synthetic_train_images, synthetic_dev_images, synthetic_train_labels, synthetic_dev_labels = train_test_split(synthetic_images, synthetic_labels, test_size=0.2, random_state=42, stratify=synthetic_labels)\n",
    "\n",
    "# Use stratified random sampling to split the data into three partitions\n",
    "synthetic_train_images, synthetic_rw_images, synthetic_train_labels, synthetic_rw_labels = train_test_split(synthetic_train_images, synthetic_train_labels, test_size=0.125, random_state=42, stratify=synthetic_train_labels)\n",
    "\n",
    "# Save the images and labels into the synthetic data folder\n",
    "np.save(f\"{synthetic_data}synthetic_train_images.npy\", synthetic_train_images)\n",
    "np.save(f\"{synthetic_data}synthetic_train_labels.npy\", synthetic_train_labels)\n",
    "\n",
    "np.save(f\"{synthetic_data}synthetic_dev_images.npy\", synthetic_dev_images)\n",
    "np.save(f\"{synthetic_data}synthetic_dev_labels.npy\", synthetic_dev_labels)\n",
    "\n",
    "np.save(f\"{synthetic_data}synthetic_rw_images.npy\", synthetic_rw_images)\n",
    "np.save(f\"{synthetic_data}synthetic_rw_labels.npy\", synthetic_rw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Load in the split synthetic data  ##############################\n",
    "\n",
    "# Load in the synthetic data and convert it to a list\n",
    "synthetic_train_images = np.load(f\"{synthetic_data}synthetic_train_images.npy\").tolist()\n",
    "synthetic_train_labels = np.load(f\"{synthetic_data}synthetic_train_labels.npy\").tolist()\n",
    "\n",
    "synthetic_dev_images = np.load(f\"{synthetic_data}synthetic_dev_images.npy\").tolist()\n",
    "synthetic_dev_labels = np.load(f\"{synthetic_data}synthetic_dev_labels.npy\").tolist()\n",
    "\n",
    "synthetic_rw_images = np.load(f\"{synthetic_data}synthetic_rw_images.npy\").tolist()\n",
    "synthetic_rw_labels = np.load(f\"{synthetic_data}synthetic_rw_labels.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backup_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
